{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3073a88a",
   "metadata": {},
   "source": [
    "# 2022-02-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773758b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap, pmap, devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66357576",
   "metadata": {},
   "source": [
    "## Large Buffer Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b26d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GpuDevice(id=0, process_index=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ca21f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 8589934592 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:         4B\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:    8.00GiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:    8.00GiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 8.00GiB\n",
      "\t\tOperator: op_name=\"jit(broadcast_in_dim)/jit(main)/broadcast_in_dim[\\n  broadcast_dimensions=()\\n  shape=(2147483648,)\\n]\" source_file=\"/tmp/ipykernel_19209/4271869147.py\" source_line=3\n",
      "\t\tXLA Label: broadcast\n",
      "\t\tShape: f32[2147483648]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 4B\n",
      "\t\tEntry Parameter Subshape: f32[]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 11:34:27.637587: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00GiB (rounded to 8589934592)requested by op \n",
      "2022-02-07 11:34:27.637646: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:474] *___________________________________________________________________________________________________\n",
      "2022-02-07 11:34:27.637695: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2089] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 8589934592 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:         4B\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:    8.00GiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:    8.00GiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 8.00GiB\n",
      "\t\tOperator: op_name=\"jit(broadcast_in_dim)/jit(main)/broadcast_in_dim[\\n  broadcast_dimensions=()\\n  shape=(2147483648,)\\n]\" source_file=\"/tmp/ipykernel_19209/4271869147.py\" source_line=3\n",
      "\t\tXLA Label: broadcast\n",
      "\t\tShape: f32[2147483648]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 4B\n",
      "\t\tEntry Parameter Subshape: f32[]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Allocate 8 GiB buffer\n",
    "    buffer = jnp.zeros(2 * 1024 ** 3)\n",
    "except RuntimeError as error:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e511ceb",
   "metadata": {},
   "source": [
    "## Just-in-Time Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8253c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
      "1.0\n",
      "-------------------------\n",
      "1.0\n",
      "-------------------------\n",
      "x = Traced<ShapedArray(int32[10])>with<DynamicJaxprTrace(level=0/1)>\n",
      "[ 0  1  4  9 16 25 36 49 64 81]\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def square(x):\n",
    "    print(f\"x = {x}\")\n",
    "    return x ** 2\n",
    "\n",
    "print(square(1.0))\n",
    "print(\"-------------------------\")\n",
    "print(square(1.0))\n",
    "print(\"-------------------------\")\n",
    "print(square(jnp.arange(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec5731",
   "metadata": {},
   "source": [
    "## JAX Misconceptions\n",
    "\n",
    "Automatic differentiation is actually done analitically. JAX knows all the operations in the `jax.numpy` namespace and is able to apply the chain rule to differentiate user code, including those with control flow. ([Source](https://youtu.be/z-WSrQDXkuM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012eeb2",
   "metadata": {},
   "source": [
    "## MPI4JAX\n",
    "\n",
    "Supported operations:\n",
    "\n",
    "- `send`\n",
    "- `recv`\n",
    "- `sendrecv`\n",
    "- `bcast`\n",
    "- `gather` (!!!)\n",
    "- `scatter`  (!!!)\n",
    "- `reduce`\n",
    "- `allgather`\n",
    "- `allreduce`\n",
    "- `alltoall`\n",
    "- `scan`\n",
    "\n",
    "### Implementation\n",
    "\n",
    "1. Python module, registering a new primitive with JAX.\n",
    "   - **Abstract evaluation rules** are used by the compiler to infer the output shapes and data types without running the actual computation.\n",
    "   - **Translation rules** determine the specific computational kernel and prepare the input buffers.\n",
    "2. A Cython function that casts raw input arguments passed by XLA to their true C type, so they can be passed on to MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259cd32",
   "metadata": {},
   "source": [
    "## FEDJAX\n",
    "\n",
    "> Federated learning is a machine learning setting where many clients collaboratively train a model\n",
    "under the orchestration of a central server, while keeping the training data decentralized. Clients\n",
    "can be either mobile devices or whole organizations depending on the task at hand.\n",
    "\n",
    "Client/server architecture with naive data partitioning:\n",
    "\n",
    "```python\n",
    "for_each_client = fedjax.for_each_client(\n",
    "    client_init=lambda server_params, _: server_params,\n",
    "    client_step=(\n",
    "        lambda params, batch: params - grad_fn(params, batch) * 0.1),\n",
    "    client_final=lambda server_params, params: server_params - params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efc43b",
   "metadata": {},
   "source": [
    "## JAX PJIT\n",
    "\n",
    "> It takes in an XLA program that represents the complete neural net, as if there is only one giant virtual device. In addition to the program, it also takes in partitioning specifications for both function inputs and outputs. The output of the XLA SPMD partitioner is an identical program for N devices that performs communications between devices through collective operations. The program only compiles once per host. **Pjit is the API exposed for the XLA SPMD partitioner in JAX**. ([Source](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html))\n",
    "\n",
    "![XLA SPMD](https://jax.readthedocs.io/en/latest/_images/xla_spmd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9251af9",
   "metadata": {},
   "source": [
    "## JAX PMAP\n",
    "\n",
    "TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
