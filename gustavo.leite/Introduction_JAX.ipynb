{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17679e44",
   "metadata": {},
   "source": [
    "# Google JAX\n",
    "\n",
    "JAX (**J**ust **A**fter e**X**ecution) is basically Numpy for CPU, GPU, and TPU. It is a Python library developed by Google which replicates the Numpy API but is capable of offloading numerical computation to other devices. On top of that, JAX can just-in-time compile and optimize pure functions for extra speed. Users also can compute the gradient (*i.e.* derivative) of Python functions automatically. Working with JAX is almost as easy as working with Numpy. It's primary use is for machine learning workloads, but is not limited by that!\n",
    "\n",
    "In the next section we will show the power and simplicity of JAX with a simple example and from there we will build increasingly more complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d230764",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "In this section we will show a motivating example using the well-known `sigmoid` function! But first, let us get the imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed81600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax essential imports\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap, pmap, make_jaxpr\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use seaborn defaults for plotting\n",
    "sns.set()\n",
    "\n",
    "# Size of the array used for testing\n",
    "SIZE = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08042b51",
   "metadata": {},
   "source": [
    "We define the `sigmoid` function and its derivative `sigmoid_prime` using `numpy` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30016f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function implemented with numpy.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"First derivative of the sigmoid function implemented with numpy.\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2691",
   "metadata": {},
   "source": [
    "Let's generate an input array and feed it to both functions and measure the average time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652746be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821 µs ± 7.26 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "966 µs ± 26.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Generates an array of SIZE elements with values uniformly ranging from -10 to 10.\n",
    "z = np.linspace(-10.0, 10.0, SIZE)\n",
    "\n",
    "t_s  = %timeit -o sigmoid(z)\n",
    "t_sp = %timeit -o sigmoid_prime(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef3871",
   "metadata": {},
   "source": [
    "The time on my machine are:\n",
    "\n",
    "- `sigmoid`: 779 us\n",
    "- `sigmoid_prime`: 848 us\n",
    "\n",
    "Now let us reproduce the same example using JAX, then we will compare the running time of both examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657c2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sigmoid_jax(z):\n",
    "    \"\"\"Sigmoid function implemented with JAX and JIT compiled.\"\"\"\n",
    "    # Notice the use of `jnp` here instead of `np`.\n",
    "    return 1 / (1 + jnp.exp(-z))\n",
    "\n",
    "# Create a JIT'ed and vectorized gradient function\n",
    "sigmoid_prime_jax = jit(vmap(grad(sigmoid_jax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3851e",
   "metadata": {},
   "source": [
    "Well, that is quite a bit of things going on here. Let's break it down.\n",
    "\n",
    "On line 1, we use the `jax.jit` decorator in our `sigmoid_jax` function. That means that the first time we invoke this function, it will be compiled to our backend (CPU, in the case of this documen) and executed natively with improved performance!\n",
    "\n",
    "Note on line 3 that we are using the JAX version of numpy via the namespace `jnp`. JAX replicates the numpy interface very closely, so, in practice, just adding the letter J to your numpy code should be sufficient!\n",
    "\n",
    "One line 5 we compute the derivative of `sigmoid_jax` with respect to `z` by composing a couple of functions:\n",
    "\n",
    "1. The `jax.grad` function is an operator that returns the gradient of a function. By default, it takes the gradient with respect to the first paramter (there is only one in this case, `z`) but we could change that if we wanted!\n",
    "\n",
    "2. The gradient function is not vectorized, meaning that we could input only one \"point\" at a time to obtain its gradient. In this case, `jax.vmap` **vectorizes** the input function.\n",
    "\n",
    "3. Finally, we use `jax.jit` on this new function to just-in-time compiler (and optimize) it.\n",
    "\n",
    "Now we are going to time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d917cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.2 µs ± 3.94 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "43.4 µs ± 1.01 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Same as before, but this time we use the JAX API.\n",
    "z = jnp.linspace(-10.0, 10.0, SIZE)\n",
    "\n",
    "t_sj  = %timeit -o sigmoid_jax(z).block_until_ready()\n",
    "t_spj = %timeit -o sigmoid_prime_jax(z).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5301c3a",
   "metadata": {},
   "source": [
    "On my machine the results read:\n",
    "\n",
    "- `sigmoid_jax`: 87.5 us\n",
    "- `sigmoid_prime_jax`: 99.2 us\n",
    "\n",
    "Pay attention to the use of `block_until_ready()`. As stated before, JAX support several backends (i.e. devices) and some computation may run asynchronously from the host. For this reason we must explicitly tell Python to wait for the results to be ready.\n",
    "\n",
    "Now let's calculate the speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9664fdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup sigmoid       = 18.58x\n",
      "Speedup sigmoid prime = 22.25x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Speedup sigmoid       = {t_s.average  / t_sj.average :.2f}x\")\n",
    "print(f\"Speedup sigmoid prime = {t_sp.average / t_spj.average:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3abb2",
   "metadata": {},
   "source": [
    "More than 8x speedup, that's a lot! And we barely did any work. Note that, in this case, both Numpy and JAX are running on the CPU, but because of the JIT and vectorization capabilities, we end up with much faster code using the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4792d714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = xla_call[\n",
       "      call_jaxpr={ lambda ; c:f32[]. let\n",
       "          d:f32[] = neg c\n",
       "          e:f32[] = exp d\n",
       "          f:f32[] = add e 1.0\n",
       "          g:f32[] = div 1.0 f\n",
       "        in (g,) }\n",
       "      name=sigmoid_jax\n",
       "    ] a\n",
       "  in (b,) }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(sigmoid_jax)(0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
